{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPTNeo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMdJtd+xICZ9sCUZ2quj5yA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joexu22/clifford/blob/master/GPTNeo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRoXPyAR5uoT"
      },
      "source": [
        "Me Setting Up GPTNeo\n",
        "\n",
        "[GPTNeo](https://github.com/EleutherAI/GPTNeo) by [EleutherAI](eleuther.ai)\n",
        "\n",
        "[Eleuther's Discord](https://discord.gg/BK2v3EJ) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_3Xt4p-6Tp-"
      },
      "source": [
        "Setup Instructions\n",
        "\n",
        "1. You need to turn on TPU Runtime\n",
        "  - Go to \"Runtime\" -> Choose \"Change runtime type\" and select \"TPU\" under \"hardware accelerator\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ud_zrtjQ5RuB",
        "outputId": "53d4b248-1226-46aa-f9c6-2574e038d6fe"
      },
      "source": [
        "#@title Setup\n",
        "%tensorflow_version 2.x\n",
        "!git clone https://github.com/EleutherAI/GPTNeo\n",
        "%cd GPTNeo\n",
        "!pip3 install -q -r requirements.txt"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'GPTNeo'...\n",
            "remote: Enumerating objects: 3747, done.\u001b[K\n",
            "remote: Counting objects: 100% (138/138), done.\u001b[K\n",
            "remote: Compressing objects: 100% (68/68), done.\u001b[K\n",
            "remote: Total 3747 (delta 77), reused 121 (delta 69), pack-reused 3609\u001b[K\n",
            "Receiving objects: 100% (3747/3747), 1.44 MiB | 10.76 MiB/s, done.\n",
            "Resolving deltas: 100% (2167/2167), done.\n",
            "/content/GPTNeo\n",
            "\u001b[K     |████████████████████████████████| 368kB 6.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 14.4MB 18.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 51.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 394.7MB 41kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4MB 39.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 44.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.5MB 47.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 7.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 184kB 53.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.2MB 37.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0MB 45.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 712kB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 41.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 901kB 37.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 7.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 286kB 49.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 7.5MB/s \n",
            "\u001b[?25h  Building wheel for tpunicorn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ring (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wirerope (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkxwszQDrPca"
      },
      "source": [
        "#@title Sampling Only Dataset\n",
        "\n",
        "dataset = 'Sampling_Only'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57bpcVUG8R3Q"
      },
      "source": [
        "#@title Use Pretrained Model\n",
        "pretrained_model = None"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSaRCAot-1er"
      },
      "source": [
        "Notes -\n",
        "\n",
        "Using TPUs requires Cloud Filesystems\n",
        "\n",
        "Using Google Cloud: https://console.cloud.google.com/\n",
        "\n",
        "Making Google Cloud Bucket: https://console.cloud.google.com/storage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GTtIPSa9vaQ",
        "outputId": "3d2773ab-4f33-45f2-8ef0-acf9af40a861"
      },
      "source": [
        "#@title Connect To Google Cloud Bucket - Step 1 (Authentication)\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "!gcloud init"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Welcome! This command will take you through the configuration of gcloud.\n",
            "\n",
            "Settings from your current configuration [default] are:\n",
            "component_manager:\n",
            "  disable_update_check: 'True'\n",
            "compute:\n",
            "  gce_metadata_read_timeout_sec: '0'\n",
            "core:\n",
            "  account: kiro66500@gmail.com\n",
            "\n",
            "Pick configuration to use:\n",
            " [1] Re-initialize this configuration [default] with new settings \n",
            " [2] Create a new configuration\n",
            "Please enter your numeric choice:  1\n",
            "\n",
            "Your current configuration has been set to: [default]\n",
            "\n",
            "You can skip diagnostics next time by using the following flag:\n",
            "  gcloud init --skip-diagnostics\n",
            "\n",
            "Network diagnostic detects and fixes local network connection issues.\n",
            "Reachability Check passed.\n",
            "Network diagnostic passed (1/1 checks passed).\n",
            "\n",
            "Choose the account you would like to use to perform operations for \n",
            "this configuration:\n",
            " [1] kiro66500@gmail.com\n",
            " [2] Log in with a new account\n",
            "Please enter your numeric choice:  1\n",
            "\n",
            "You are logged in as: [kiro66500@gmail.com].\n",
            "\n",
            "Pick cloud project to use: \n",
            " [1] databasetest-536c2\n",
            " [2] my-new-gptneo-project\n",
            " [3] sinuous-photon-311106\n",
            " [4] Create a new project\n",
            "Please enter numeric choice or text value (must exactly match list \n",
            "item):  3\n",
            "\n",
            "Your current project has been set to: [sinuous-photon-311106].\n",
            "\n",
            "Not setting default zone/region (this feature makes it easier to use\n",
            "[gcloud compute] by setting an appropriate default value for the\n",
            "--zone and --region flag).\n",
            "See https://cloud.google.com/compute/docs/gcloud-compute section on how to set\n",
            "default compute region and zone manually. If you would like [gcloud init] to be\n",
            "able to do this for you the next time you run it, make sure the\n",
            "Compute Engine API is enabled for your project on the\n",
            "https://console.developers.google.com/apis page.\n",
            "\n",
            "Your Google Cloud SDK is configured and ready to use!\n",
            "\n",
            "* Commands that require authentication will use kiro66500@gmail.com by default\n",
            "* Commands will reference project `sinuous-photon-311106` by default\n",
            "Run `gcloud help config` to learn how to change individual settings\n",
            "\n",
            "This gcloud configuration is called [default]. You can create additional configurations if you work with multiple accounts and/or projects.\n",
            "Run `gcloud topic configurations` to learn more.\n",
            "\n",
            "Some things to try next:\n",
            "\n",
            "* Run `gcloud --help` to see the Cloud Platform services you can interact with. And run `gcloud help COMMAND` to get help on any gcloud command.\n",
            "* Run `gcloud topic --help` to learn about advanced features of the SDK like arg files and output formatting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0KCwx66_ele"
      },
      "source": [
        "#@title Use Cloud Bucket - Step 2 (Locate/Use Bucket Location)\n",
        "path_to_cloud_bucket = 'gs://my-machine-learning-bucket' #@param {type:\"string\"}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCW9erQu8SSC"
      },
      "source": [
        "#@title Use Dataset (Currently: Sampling Only)\n",
        "import os\n",
        "dataset = \"Sampling_Only\"\n",
        "pass"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zABLzgReCtMw",
        "outputId": "4f0b836c-8601-4904-f8dd-c244411a3ac9"
      },
      "source": [
        "#@title Sampling Only Configuration - Step 1 (CD to GPTNeo Directory)\n",
        "# strange how they don't let me use %cd and %%writefile together\n",
        "%cd /content/GPTNeo"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/GPTNeo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuUpdO4mDugu",
        "outputId": "66f3bdee-480d-4806-dac5-012f40440412"
      },
      "source": [
        "#@title Sampling Only Configuration - Step 2 (Write File)\n",
        "%%writefile configs/dataset_configs/Sampling_Only.json\n",
        "\n",
        "{\n",
        "  \"path\": \"gs://my-machine-learning-bucket/datasets/Sampling_Only/Sampling_Only*.tfrecords\",\n",
        "  \"eval_path\": \"\",\n",
        "  \"n_vocab\": 50256,\n",
        "  \"tokenizer_is_pretrained\": true,\n",
        "  \"tokenizer_path\": \"gpt2\",\n",
        "  \"eos_id\": 50256,\n",
        "  \"padding_id\": 50257\n",
        "}\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing configs/dataset_configs/Sampling_Only.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2hOwCEvRVtn",
        "outputId": "afb9f1ab-575d-4baa-928a-b31c8a4d615e"
      },
      "source": [
        "#@title Sampling Only Configuration - Step 3 (Write Model Config)\n",
        "%%writefile configs/GPT3_2-7B.json\n",
        "\n",
        "{\n",
        "    \"n_head\": 16,\n",
        "    \"n_vocab\": 50257,\n",
        "    \"embed_dropout\": 0,\n",
        "    \"lr\": 0.0002,\n",
        "    \"lr_decay\": \"cosine\",\n",
        "    \"warmup_steps\": 3000,\n",
        "    \"beta1\": 0.9,\n",
        "    \"beta2\": 0.95,\n",
        "    \"epsilon\": 1e-8,\n",
        "    \"opt_name\": \"adam\",\n",
        "    \"weight_decay\": 0,\n",
        "    \"train_batch_size\": 256,\n",
        "    \"attn_dropout\": 0,\n",
        "    \"train_steps\": 600000,\n",
        "    \"eval_steps\": 0,\n",
        "    \"predict_steps\": 1,\n",
        "    \"res_dropout\": 0,\n",
        "    \"eval_batch_size\": 4,\n",
        "    \"predict_batch_size\": 1,\n",
        "    \"iterations\": 100,\n",
        "    \"n_embd\": 2048,\n",
        "    \"datasets\": [[\"pile\", null, null, null]],\n",
        "    \"model\": \"GPT\",\n",
        "    \"model_path\": \"gs://my-machine-learning-bucket/GPT3_2-7B\",\n",
        "    \"n_ctx\": 2048,\n",
        "    \"n_layer\": 24,\n",
        "    \"scale_by_depth\": true,\n",
        "    \"scale_by_in\": false,\n",
        "    \"attention_types\" :  [[[\"global\", \"local\"],12]],\n",
        "    \"mesh_shape\": \"x:4,y:2\",\n",
        "    \"layout\": \"intermediate_expanded:x,heads:x,vocab:n_vocab,memory_length:y,embd:y\",\n",
        "    \"activation_function\": \"gelu\",\n",
        "    \"recompute_grad\": true,\n",
        "    \"gradient_clipping\": 1.0,\n",
        "    \"tokens_per_mb_per_replica\": 2048,\n",
        "    \"precision\": \"bfloat16\"\n",
        "}"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing configs/GPT3_2-7B.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHjuX3H9GsZL"
      },
      "source": [
        "# @title Get Pretrained Models - Step 1 (Uncomment If Missing Weights)\n",
        "pretrained_model = 'GPT3_2-7B' #@param [\"GPT3_XL\", \"GPT3_2-7B\"]\n",
        "# !wget -m -np -c -U \"eye02\" -w 2 -R \"index.html*\" \"https://the-eye.eu/public/AI/gptneo-release/$pretrained_model/\"\n",
        "path_to_local_weights = f\"/content/GPTNeo/the-eye.eu/public/AI/gptneo-release/{pretrained_model}\"\n",
        "\n",
        "# URL = f\"http://eaidata.bmk.sh/data/gptneo-release/{pretrained_model}/\"\n",
        "# FOLDER_NAME = \"GPT3_XL\"\n",
        "# !curl $URL | grep -i \"</a>\" | sed -n 's/.*href=\"\\([^\"]*\\).*/\\1/p' | sed \"s|^|$URL|\" | xargs -n 1 -P 4 wget -P $pretrained_model\n",
        "# path_to_local_weights = pretrained_model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvruIloKKWgM"
      },
      "source": [
        "# @title Get Pretrained Models - Step 2 (Uncomment If Missing Weights)\n",
        "\n",
        "# upload to your bucket\n",
        "bucket_base = \"gs://\" + path_to_cloud_bucket.replace('gs://', '').split('/')[0]\n",
        "# !gsutil -m cp -r $path_to_local_weights $bucket_base"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyjfFKc1Khn8",
        "outputId": "f1f91f74-6179-4897-c7c4-355d82677819"
      },
      "source": [
        "# @title Confirm Bucket\n",
        "!gsutil ls $bucket_base"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gs://my-machine-learning-bucket/GPT3_2-7B/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1TsW9Lioy-j",
        "outputId": "ad5cfc7f-ed7f-4ba9-8416-0137ede2af94"
      },
      "source": [
        "# @title Get The GPT3_2-7B Config\n",
        "%cd /content/GPTNeo\n",
        "!wget \"https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/config.json\" -P configs/local_configs/"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/GPTNeo\n",
            "--2021-05-10 05:33:10--  https://the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/config.json\n",
            "Resolving the-eye.eu (the-eye.eu)... 162.213.130.242\n",
            "Connecting to the-eye.eu (the-eye.eu)|162.213.130.242|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 937 [application/json]\n",
            "Saving to: ‘configs/local_configs/config.json’\n",
            "\n",
            "config.json         100%[===================>]     937  --.-KB/s    in 0s      \n",
            "\n",
            "2021-05-10 05:33:10 (133 MB/s) - ‘configs/local_configs/config.json’ saved [937/937]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_U5WCx4LLEe",
        "outputId": "c250333b-a2d6-4822-c39b-08a63d94ac10"
      },
      "source": [
        "# @title Modify config for colab. \n",
        "  \n",
        "import json\n",
        "from pprint import pprint\n",
        "\n",
        "path_to_model = \"\" #@param {type:\"string\"}\n",
        "batch_size = 8 #@param {type:\"integer\"}\n",
        "dset = \"pile\"  #@param {type:\"string\"}\n",
        "mesh_shape = \"x:4,y:2\" #@param {type:\"string\"}\n",
        "train_steps = 1000 #@param {type:\"integer\"}\n",
        "steps_per_checkpoint = 500 #@param {type:\"integer\"}\n",
        "start_step = 400000 if pretrained_model == \"GPT3_2-7B\" else 362000\n",
        "\n",
        "if path_to_model == \"\":\n",
        "  path_to_model = f'{bucket_base.strip(\"/\")}/{pretrained_model}'\n",
        "print(f'MODEL PATH: {path_to_model}\\n')\n",
        "\n",
        "if dset == \"\" and dataset != \"Sampling_Only\":\n",
        "  dset = dataset\n",
        "elif dataset is None and dset == \"\":\n",
        "  dset = \"pile\"\n",
        "\n",
        "def pad_to_multiple_of(n, mult):\n",
        "  \"\"\"\n",
        "  pads n to a multiple of mult\n",
        "  \"\"\"\n",
        "  extra = n % mult\n",
        "  if extra > 0:\n",
        "      n = n + mult - extra\n",
        "  return n\n",
        "\n",
        "with open(f'/content/GPTNeo/configs/local_configs/config.json', 'r') as f:\n",
        "  data = json.load(f)\n",
        "  pprint(data)\n",
        "  dset_val = [[dset, None, None, None]] if dset != \"\" else data[\"datasets\"]\n",
        "  mods = {\n",
        "          \"mesh_shape\": mesh_shape,\n",
        "          \"layout\": \"intermediate_expanded:x,heads:x,memory_length:y,embd:y\",\n",
        "          \"model_path\": path_to_model,\n",
        "          \"datasets\": dset_val,\n",
        "          \"train_steps\": start_step + train_steps,\n",
        "          \"eval_steps\": 0,\n",
        "          \"train_batch_size\": batch_size,\n",
        "          \"predict_batch_size\": batch_size\n",
        "        }\n",
        "  data.update(mods)\n",
        "  print('\\n--->\\n')\n",
        "  pprint(data)\n",
        "  with open(f'configs/{pretrained_model}.json', 'w') as outfile:\n",
        "    json.dump(data, outfile, indent=2)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MODEL PATH: gs://my-machine-learning-bucket/GPT3_2-7B\n",
            "\n",
            "{'activation_function': 'gelu',\n",
            " 'ada_epsilon1': '1e-30',\n",
            " 'ada_epsilon2': 0.001,\n",
            " 'attention_types': [[['global', 'local'], 16]],\n",
            " 'attn_dropout': 0,\n",
            " 'beta1': 0.9,\n",
            " 'beta2': 0.95,\n",
            " 'datasets': [['pile', None, None, None]],\n",
            " 'embed_dropout': 0,\n",
            " 'eos_id': 50256,\n",
            " 'epsilon': 1e-08,\n",
            " 'eval_batch_size': 128,\n",
            " 'eval_steps': 10,\n",
            " 'gradient_clipping': 1.0,\n",
            " 'iterations': 500,\n",
            " 'layout': 'batch:x,embd:y',\n",
            " 'lr': 0.00016,\n",
            " 'lr_decay': 'cosine',\n",
            " 'lr_decay_end': 300000,\n",
            " 'mesh_shape': 'x:64,y:4',\n",
            " 'model_path': 'gs://neo-d/models/GPT3_2-7B',\n",
            " 'n_ctx': 2048,\n",
            " 'n_embd': 2560,\n",
            " 'n_head': 20,\n",
            " 'n_layer': 32,\n",
            " 'n_vocab': 50257,\n",
            " 'opt_name': 'adam',\n",
            " 'padding_id': 50257,\n",
            " 'predict_batch_size': 1,\n",
            " 'predict_steps': 0,\n",
            " 'recompute_grad': True,\n",
            " 'res_dropout': 0,\n",
            " 'scale_by_depth': True,\n",
            " 'scale_by_in': False,\n",
            " 'tokens_per_mb_per_replica': 4096,\n",
            " 'train_batch_size': 512,\n",
            " 'train_steps': 400000,\n",
            " 'warmup_steps': 3000,\n",
            " 'weight_decay': 0}\n",
            "\n",
            "--->\n",
            "\n",
            "{'activation_function': 'gelu',\n",
            " 'ada_epsilon1': '1e-30',\n",
            " 'ada_epsilon2': 0.001,\n",
            " 'attention_types': [[['global', 'local'], 16]],\n",
            " 'attn_dropout': 0,\n",
            " 'beta1': 0.9,\n",
            " 'beta2': 0.95,\n",
            " 'datasets': [['pile', None, None, None]],\n",
            " 'embed_dropout': 0,\n",
            " 'eos_id': 50256,\n",
            " 'epsilon': 1e-08,\n",
            " 'eval_batch_size': 128,\n",
            " 'eval_steps': 0,\n",
            " 'gradient_clipping': 1.0,\n",
            " 'iterations': 500,\n",
            " 'layout': 'intermediate_expanded:x,heads:x,memory_length:y,embd:y',\n",
            " 'lr': 0.00016,\n",
            " 'lr_decay': 'cosine',\n",
            " 'lr_decay_end': 300000,\n",
            " 'mesh_shape': 'x:4,y:2',\n",
            " 'model_path': 'gs://my-machine-learning-bucket/GPT3_2-7B',\n",
            " 'n_ctx': 2048,\n",
            " 'n_embd': 2560,\n",
            " 'n_head': 20,\n",
            " 'n_layer': 32,\n",
            " 'n_vocab': 50257,\n",
            " 'opt_name': 'adam',\n",
            " 'padding_id': 50257,\n",
            " 'predict_batch_size': 8,\n",
            " 'predict_steps': 0,\n",
            " 'recompute_grad': True,\n",
            " 'res_dropout': 0,\n",
            " 'scale_by_depth': True,\n",
            " 'scale_by_in': False,\n",
            " 'tokens_per_mb_per_replica': 4096,\n",
            " 'train_batch_size': 8,\n",
            " 'train_steps': 401000,\n",
            " 'warmup_steps': 3000,\n",
            " 'weight_decay': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ts_f49zeLzaC",
        "outputId": "c71c3e47-43d3-4ebb-d7ec-6546227735fd"
      },
      "source": [
        "# @title Sample Text\n",
        "%%writefile example_prompt.txt\n",
        "\n",
        "So, GPTNeo. I have set you up.\n",
        "What are your further instructions?\n",
        "\n",
        "List Them Here:\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing example_prompt.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLMTbrx8Zdln"
      },
      "source": [
        "# @title Quick Train\n",
        "# !python3 main.py --model $pretrained_model --steps_per_checkpoint $steps_per_checkpoint --tpu colab"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfDsWYvpMOdU",
        "outputId": "0d978096-8e3e-4812-a581-7a3fecdbb226"
      },
      "source": [
        "# @title Run Code\n",
        "!python3 main.py --model $pretrained_model --steps_per_checkpoint 500 --tpu colab --predict --prompt example_prompt.txt"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-10 05:33:30.096122: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "Current step 400000\n",
            "Downloading: 100% 1.04M/1.04M [00:00<00:00, 5.94MB/s]\n",
            "Downloading: 100% 456k/456k [00:00<00:00, 3.27MB/s]\n",
            "Downloading: 100% 1.36M/1.36M [00:00<00:00, 7.36MB/s]\n",
            "Saving config to gs://my-machine-learning-bucket/GPT3_2-7B\n",
            "2021-05-10 05:33:40.025131: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-05-10 05:33:40.027351: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-05-10 05:33:40.039784: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-05-10 05:33:40.039854: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (a58810b328a3): /proc/driver/nvidia/version does not exist\n",
            "2021-05-10 05:33:40.305379: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:196] None of the MLIR optimization passes are enabled (registered 0 passes)\n",
            "Done!\n",
            "params = defaultdict(<function fetch_model_params.<locals>.<lambda> at 0x7f32b5be9320>, {'n_head': 20, 'n_vocab': 50257, 'embed_dropout': 0, 'lr': 0.00016, 'lr_decay': 'cosine', 'warmup_steps': 3000, 'beta1': 0.9, 'beta2': 0.95, 'epsilon': 1e-08, 'ada_epsilon1': '1e-30', 'ada_epsilon2': 0.001, 'opt_name': 'adam', 'weight_decay': 0, 'train_batch_size': 8, 'attn_dropout': 0, 'train_steps': 401000, 'lr_decay_end': 300000, 'eval_steps': 0, 'predict_steps': 0, 'res_dropout': 0, 'eval_batch_size': 128, 'predict_batch_size': 8, 'iterations': 500, 'n_embd': 2560, 'datasets': [['pile', None, None, None]], 'model_path': 'gs://my-machine-learning-bucket/GPT3_2-7B', 'n_ctx': 2048, 'n_layer': 32, 'scale_by_depth': True, 'scale_by_in': False, 'attention_types': ['global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local', 'global', 'local'], 'mesh_shape': 'x:4,y:2', 'layout': 'intermediate_expanded:x,heads:x,memory_length:y,embd:y', 'activation_function': 'gelu', 'recompute_grad': True, 'gradient_clipping': 1.0, 'tokens_per_mb_per_replica': 4096, 'padding_id': 50257, 'eos_id': 50256, 'dataset_configs': {'pile': {'n_vocab': 50257, 'path': 'gs://neo-datasets/pile/pile_*.tfrecords', 'eval_path': 'gs://neo-datasets/pile_val.tfrecords', 'tokenizer_is_pretrained': True, 'tokenizer_path': 'gpt2', 'eos_id': 50256, 'padding_id': 50257}}, 'mlm_training': False, 'causal': True, 'num_cores': 8, 'auto_layout': False, 'auto_layout_and_mesh_shape': False, 'use_tpu': True, 'gpu_ids': ['device:GPU:0'], 'steps_per_checkpoint': 500, 'predict': True, 'model': 'GPT', 'export': False, 'sampling_use_entmax': False, 'moe_layers': None, 'slow_sampling': False})\n",
            "Using config: {'_model_dir': 'gs://my-machine-learning-bucket/GPT3_2-7B', '_tf_random_seed': None, '_save_summary_steps': 500, '_save_checkpoints_steps': None, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
            "cluster_def {\n",
            "  job {\n",
            "    name: \"worker\"\n",
            "    tasks {\n",
            "      key: 0\n",
            "      value: \"10.89.81.26:8470\"\n",
            "    }\n",
            "  }\n",
            "}\n",
            "isolate_session_state: true\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({'worker': ['10.89.81.26:8470']}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': 'grpc://10.89.81.26:8470', '_evaluation_master': 'grpc://10.89.81.26:8470', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=500, num_shards=8, num_cores_per_replica=1, per_host_input_for_training=4, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1, experimental_allow_per_host_v2_parallel_get_next=False, experimental_feed_hook=None), '_cluster': <tensorflow.python.distribute.cluster_resolver.tpu.tpu_cluster_resolver.TPUClusterResolver object at 0x7f32b5bce310>}\n",
            "_TPUContext: eval_on_tpu True\n",
            "Predictions generated\n",
            "Querying Tensorflow master (grpc://10.89.81.26:8470) for TPU system metadata.\n",
            "2021-05-10 05:33:41.344394: W tensorflow/core/distributed_runtime/rpc/grpc_session.cc:373] GrpcSession::ListDevices will initialize the session with an empty graph and other defaults because the session has not yet been created.\n",
            "Initializing TPU system (master: grpc://10.89.81.26:8470) to fetch topology for model parallelism. This might take a while.\n",
            "Found TPU system:\n",
            "*** Num TPU Cores: 8\n",
            "*** Num TPU Workers: 1\n",
            "*** Num TPU Cores Per Worker: 8\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, -1, 8596324457192516229)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 2996750728799973166)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, -6626740660040963930)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 3304308222191105619)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 3435944977994337594)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, -3771532677631835771)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 3184675503300440366)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, -2236501836534570239)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 2548498408937370468)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 8589934592, 4242868443499265212)\n",
            "*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 2691847599647710556)\n",
            "Calling model_fn.\n",
            "num_cores_per_replica: 1\n",
            "computation_shape: [1, 1, 1, 1]\n",
            "num_replicas: 8\n",
            "device_assignment.topology.device_coordinates: [[[0 0 0 0]\n",
            "  [0 0 0 1]\n",
            "  [1 0 0 0]\n",
            "  [1 0 0 1]\n",
            "  [0 1 0 0]\n",
            "  [0 1 0 1]\n",
            "  [1 1 0 0]\n",
            "  [1 1 0 1]]]\n",
            "device_assignment.core_assignment: [[[0 0 0 0]]\n",
            "\n",
            " [[0 0 0 1]]\n",
            "\n",
            " [[1 0 0 0]]\n",
            "\n",
            " [[1 0 0 1]]\n",
            "\n",
            " [[0 1 0 0]]\n",
            "\n",
            " [[0 1 0 1]]\n",
            "\n",
            " [[1 1 0 0]]\n",
            "\n",
            " [[1 1 0 1]]]\n",
            "2021-05-10 05:33:50.421309: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "device_list = ['/job:worker/task:0/device:CPU:0']\n",
            "SimdMeshImpl ignoring devices ['', '', '', '', '', '', '', '']\n",
            "SimdMeshImpl init: Shape[x=4, y=2] LayoutRules{('intermediate_expanded', 'x'), ('embd', 'y'), ('heads', 'x'), ('memory_length', 'y')}\n",
            "Device Assignment: <tensorflow.python.tpu.device_assignment.DeviceAssignment object at 0x7f32b03ee250>\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Defaulting to GELU activation (see here: https://arxiv.org/abs/1606.08415)\n",
            "Create pnum_tensor\n",
            "Variable gpt2/h0/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h0/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h0/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h0/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h0/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h0/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h1/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h1/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h1/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h1/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h1/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h1/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h10/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h10/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h10/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h10/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h10/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h10/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h11/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h11/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h11/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h11/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h11/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h11/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h12/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h12/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h12/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h12/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h12/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h12/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h13/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h13/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h13/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h13/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h13/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h13/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h14/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h14/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h14/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h14/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h14/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h14/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h15/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h15/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h15/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h15/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h15/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h15/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h16/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h16/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h16/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h16/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h16/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h16/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h17/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h17/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h17/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h17/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h17/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h17/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h18/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h18/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h18/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h18/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h18/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h18/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h19/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h19/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h19/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h19/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h19/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h19/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h2/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h2/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h2/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h2/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h2/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h2/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h20/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h20/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h20/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h20/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h20/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h20/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h21/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h21/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h21/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h21/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h21/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h21/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h22/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h22/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h22/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h22/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h22/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h22/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h23/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h23/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h23/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h23/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h23/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h23/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h24/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h24/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h24/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h24/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h24/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h24/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h25/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h25/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h25/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h25/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h25/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h25/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h26/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h26/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h26/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h26/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h26/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h26/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h27/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h27/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h27/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h27/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h27/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h27/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h28/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h28/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h28/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h28/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h28/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h28/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h29/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h29/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h29/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h29/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h29/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h29/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h3/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h3/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h3/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h3/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h3/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h3/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h30/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h30/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h30/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h30/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h30/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h30/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h31/attn/k                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h31/attn/o                                              size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h31/attn/q                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h31/attn/v                                              size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h31/mlp/conv1d_main/c_fc/kernel                         size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h31/mlp/conv1d_main/c_proj/kernel                       size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h4/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h4/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h4/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h4/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h4/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h4/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h5/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h5/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h5/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h5/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h5/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h5/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h6/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h6/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h6/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h6/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h6/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h6/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h7/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h7/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h7/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h7/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h7/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h7/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h8/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h8/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h8/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h8/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h8/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h8/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/h9/attn/k                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h9/attn/o                                               size 6553600      slice_size 819200       Shape[heads=2560, embd=2560]                                \n",
            "Variable gpt2/h9/attn/q                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h9/attn/v                                               size 6553600      slice_size 819200       Shape[embd=2560, heads=2560]                                \n",
            "Variable gpt2/h9/mlp/conv1d_main/c_fc/kernel                          size 26214400     slice_size 3276800      Shape[embd=2560, intermediate_expanded=10240]               \n",
            "Variable gpt2/h9/mlp/conv1d_main/c_proj/kernel                        size 26214400     slice_size 3276800      Shape[intermediate_expanded=10240, embd=2560]               \n",
            "Variable gpt2/wpe                                                     size 5242880      slice_size 2621440      Shape[embed_sequence=2048, embd=2560]                       \n",
            "Variable gpt2/wte                                                     size 128657920    slice_size 64328960     Shape[vocab=50257, embd=2560]                               \n",
            "Variable stacked/gpt2/h0/mlp/conv1d_main/c_fc/bias                    size 256000       slice_size 64000        Shape[stacked=25, intermediate_expanded=10240]              \n",
            "    gpt2/h0/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h1/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h2/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h3/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h4/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h5/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h6/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h7/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h8/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h9/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h10/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h11/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h12/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h13/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h14/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h15/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h16/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h17/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h18/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h19/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h20/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h21/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h22/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h23/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h24/mlp/conv1d_main/c_fc/bias\n",
            "Variable stacked/gpt2/h0/norm_1/g                                     size 130560       slice_size 65280        Shape[stacked=51, embd=2560]                                \n",
            "    gpt2/h0/norm_1/g\n",
            "    gpt2/h0/norm_1/b\n",
            "    gpt2/h0/attn/compute_output_bias/o_b\n",
            "    gpt2/h0/norm_2/g\n",
            "    gpt2/h0/norm_2/b\n",
            "    gpt2/h0/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h1/norm_1/g\n",
            "    gpt2/h1/norm_1/b\n",
            "    gpt2/h1/attn/compute_output_bias/o_b\n",
            "    gpt2/h1/norm_2/g\n",
            "    gpt2/h1/norm_2/b\n",
            "    gpt2/h1/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h2/norm_1/g\n",
            "    gpt2/h2/norm_1/b\n",
            "    gpt2/h2/attn/compute_output_bias/o_b\n",
            "    gpt2/h2/norm_2/g\n",
            "    gpt2/h2/norm_2/b\n",
            "    gpt2/h2/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h3/norm_1/g\n",
            "    gpt2/h3/norm_1/b\n",
            "    gpt2/h3/attn/compute_output_bias/o_b\n",
            "    gpt2/h3/norm_2/g\n",
            "    gpt2/h3/norm_2/b\n",
            "    gpt2/h3/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h4/norm_1/g\n",
            "    gpt2/h4/norm_1/b\n",
            "    gpt2/h4/attn/compute_output_bias/o_b\n",
            "    gpt2/h4/norm_2/g\n",
            "    gpt2/h4/norm_2/b\n",
            "    gpt2/h4/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h5/norm_1/g\n",
            "    gpt2/h5/norm_1/b\n",
            "    gpt2/h5/attn/compute_output_bias/o_b\n",
            "    gpt2/h5/norm_2/g\n",
            "    gpt2/h5/norm_2/b\n",
            "    gpt2/h5/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h6/norm_1/g\n",
            "    gpt2/h6/norm_1/b\n",
            "    gpt2/h6/attn/compute_output_bias/o_b\n",
            "    gpt2/h6/norm_2/g\n",
            "    gpt2/h6/norm_2/b\n",
            "    gpt2/h6/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h7/norm_1/g\n",
            "    gpt2/h7/norm_1/b\n",
            "    gpt2/h7/attn/compute_output_bias/o_b\n",
            "    gpt2/h7/norm_2/g\n",
            "    gpt2/h7/norm_2/b\n",
            "    gpt2/h7/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h8/norm_1/g\n",
            "    gpt2/h8/norm_1/b\n",
            "    gpt2/h8/attn/compute_output_bias/o_b\n",
            "Variable stacked/gpt2/h17/norm_1/g                                    size 130560       slice_size 65280        Shape[stacked=51, embd=2560]                                \n",
            "    gpt2/h17/norm_1/g\n",
            "    gpt2/h17/norm_1/b\n",
            "    gpt2/h17/attn/compute_output_bias/o_b\n",
            "    gpt2/h17/norm_2/g\n",
            "    gpt2/h17/norm_2/b\n",
            "    gpt2/h17/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h18/norm_1/g\n",
            "    gpt2/h18/norm_1/b\n",
            "    gpt2/h18/attn/compute_output_bias/o_b\n",
            "    gpt2/h18/norm_2/g\n",
            "    gpt2/h18/norm_2/b\n",
            "    gpt2/h18/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h19/norm_1/g\n",
            "    gpt2/h19/norm_1/b\n",
            "    gpt2/h19/attn/compute_output_bias/o_b\n",
            "    gpt2/h19/norm_2/g\n",
            "    gpt2/h19/norm_2/b\n",
            "    gpt2/h19/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h20/norm_1/g\n",
            "    gpt2/h20/norm_1/b\n",
            "    gpt2/h20/attn/compute_output_bias/o_b\n",
            "    gpt2/h20/norm_2/g\n",
            "    gpt2/h20/norm_2/b\n",
            "    gpt2/h20/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h21/norm_1/g\n",
            "    gpt2/h21/norm_1/b\n",
            "    gpt2/h21/attn/compute_output_bias/o_b\n",
            "    gpt2/h21/norm_2/g\n",
            "    gpt2/h21/norm_2/b\n",
            "    gpt2/h21/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h22/norm_1/g\n",
            "    gpt2/h22/norm_1/b\n",
            "    gpt2/h22/attn/compute_output_bias/o_b\n",
            "    gpt2/h22/norm_2/g\n",
            "    gpt2/h22/norm_2/b\n",
            "    gpt2/h22/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h23/norm_1/g\n",
            "    gpt2/h23/norm_1/b\n",
            "    gpt2/h23/attn/compute_output_bias/o_b\n",
            "    gpt2/h23/norm_2/g\n",
            "    gpt2/h23/norm_2/b\n",
            "    gpt2/h23/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h24/norm_1/g\n",
            "    gpt2/h24/norm_1/b\n",
            "    gpt2/h24/attn/compute_output_bias/o_b\n",
            "    gpt2/h24/norm_2/g\n",
            "    gpt2/h24/norm_2/b\n",
            "    gpt2/h24/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h25/norm_1/g\n",
            "    gpt2/h25/norm_1/b\n",
            "    gpt2/h25/attn/compute_output_bias/o_b\n",
            "Variable stacked/gpt2/h25/mlp/conv1d_main/c_fc/bias                   size 71680        slice_size 17920        Shape[stacked=7, intermediate_expanded=10240]               \n",
            "    gpt2/h25/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h26/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h27/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h28/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h29/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h30/mlp/conv1d_main/c_fc/bias\n",
            "    gpt2/h31/mlp/conv1d_main/c_fc/bias\n",
            "Variable stacked/gpt2/h25/norm_2/g                                    size 104960       slice_size 52480        Shape[stacked=41, embd=2560]                                \n",
            "    gpt2/h25/norm_2/g\n",
            "    gpt2/h25/norm_2/b\n",
            "    gpt2/h25/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h26/norm_1/g\n",
            "    gpt2/h26/norm_1/b\n",
            "    gpt2/h26/attn/compute_output_bias/o_b\n",
            "    gpt2/h26/norm_2/g\n",
            "    gpt2/h26/norm_2/b\n",
            "    gpt2/h26/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h27/norm_1/g\n",
            "    gpt2/h27/norm_1/b\n",
            "    gpt2/h27/attn/compute_output_bias/o_b\n",
            "    gpt2/h27/norm_2/g\n",
            "    gpt2/h27/norm_2/b\n",
            "    gpt2/h27/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h28/norm_1/g\n",
            "    gpt2/h28/norm_1/b\n",
            "    gpt2/h28/attn/compute_output_bias/o_b\n",
            "    gpt2/h28/norm_2/g\n",
            "    gpt2/h28/norm_2/b\n",
            "    gpt2/h28/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h29/norm_1/g\n",
            "    gpt2/h29/norm_1/b\n",
            "    gpt2/h29/attn/compute_output_bias/o_b\n",
            "    gpt2/h29/norm_2/g\n",
            "    gpt2/h29/norm_2/b\n",
            "    gpt2/h29/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h30/norm_1/g\n",
            "    gpt2/h30/norm_1/b\n",
            "    gpt2/h30/attn/compute_output_bias/o_b\n",
            "    gpt2/h30/norm_2/g\n",
            "    gpt2/h30/norm_2/b\n",
            "    gpt2/h30/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h31/norm_1/g\n",
            "    gpt2/h31/norm_1/b\n",
            "    gpt2/h31/attn/compute_output_bias/o_b\n",
            "    gpt2/h31/norm_2/g\n",
            "    gpt2/h31/norm_2/b\n",
            "    gpt2/h31/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/ln_f/g\n",
            "    gpt2/ln_f/b\n",
            "Variable stacked/gpt2/h8/norm_2/g                                     size 130560       slice_size 65280        Shape[stacked=51, embd=2560]                                \n",
            "    gpt2/h8/norm_2/g\n",
            "    gpt2/h8/norm_2/b\n",
            "    gpt2/h8/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h9/norm_1/g\n",
            "    gpt2/h9/norm_1/b\n",
            "    gpt2/h9/attn/compute_output_bias/o_b\n",
            "    gpt2/h9/norm_2/g\n",
            "    gpt2/h9/norm_2/b\n",
            "    gpt2/h9/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h10/norm_1/g\n",
            "    gpt2/h10/norm_1/b\n",
            "    gpt2/h10/attn/compute_output_bias/o_b\n",
            "    gpt2/h10/norm_2/g\n",
            "    gpt2/h10/norm_2/b\n",
            "    gpt2/h10/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h11/norm_1/g\n",
            "    gpt2/h11/norm_1/b\n",
            "    gpt2/h11/attn/compute_output_bias/o_b\n",
            "    gpt2/h11/norm_2/g\n",
            "    gpt2/h11/norm_2/b\n",
            "    gpt2/h11/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h12/norm_1/g\n",
            "    gpt2/h12/norm_1/b\n",
            "    gpt2/h12/attn/compute_output_bias/o_b\n",
            "    gpt2/h12/norm_2/g\n",
            "    gpt2/h12/norm_2/b\n",
            "    gpt2/h12/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h13/norm_1/g\n",
            "    gpt2/h13/norm_1/b\n",
            "    gpt2/h13/attn/compute_output_bias/o_b\n",
            "    gpt2/h13/norm_2/g\n",
            "    gpt2/h13/norm_2/b\n",
            "    gpt2/h13/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h14/norm_1/g\n",
            "    gpt2/h14/norm_1/b\n",
            "    gpt2/h14/attn/compute_output_bias/o_b\n",
            "    gpt2/h14/norm_2/g\n",
            "    gpt2/h14/norm_2/b\n",
            "    gpt2/h14/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h15/norm_1/g\n",
            "    gpt2/h15/norm_1/b\n",
            "    gpt2/h15/attn/compute_output_bias/o_b\n",
            "    gpt2/h15/norm_2/g\n",
            "    gpt2/h15/norm_2/b\n",
            "    gpt2/h15/mlp/conv1d_main/c_proj/bias\n",
            "    gpt2/h16/norm_1/g\n",
            "    gpt2/h16/norm_1/b\n",
            "    gpt2/h16/attn/compute_output_bias/o_b\n",
            "    gpt2/h16/norm_2/g\n",
            "    gpt2/h16/norm_2/b\n",
            "    gpt2/h16/mlp/conv1d_main/c_proj/bias\n",
            "Trainable Variables            count: 200     Total size: 2651307520       Total slice_size: 381853440      \n",
            "All Variables                  count: 200     Total size: 2651307520       Total slice_size: 381853440      \n",
            "Counters:\n",
            "allreduce: 3.36e+10\n",
            " allreduce/[0]: 1.07e+10\n",
            "  allreduce/[0]/einsum_op: 1.07e+10\n",
            " allreduce/[1]: 2.29e+10\n",
            "  allreduce/[1]/einsum_op: 2.28e+10\n",
            "  allreduce/[1]/reduce_op: 3.8e+07\n",
            "einsum: 6.37e+13\n",
            "einsum_unique: 4.96e+13\n",
            "output: 3.99e+11\n",
            " output/AddOperation: 1.13e+11\n",
            " output/BinaryOpWithBroadcasting: 6.88e+08\n",
            " output/BroadcastOperation: 1.08e+10\n",
            " output/ConcatOperation: 5.37e+09\n",
            " output/Constant: 2.62e+05\n",
            " output/EinsumOperation: 1.12e+11\n",
            " output/ImportOperation: 2.62e+05\n",
            " output/OneHotOperation: 6.62e+09\n",
            " output/RangeOperation: 3.19e+05\n",
            " output/ReduceOperation: 5.9e+07\n",
            " output/ReshapeOperation: 2.01e+10\n",
            " output/ScalarAddOperation: 1.07e+10\n",
            " output/ScalarMultiplyOperation: 3.77e+10\n",
            " output/ShiftOperation: 2.68e+09\n",
            " output/SlicewiseOperation: 5.42e+10\n",
            " output/StackedVariable: 2.64e+06\n",
            " output/StopGradient: 1.61e+10\n",
            " output/UnstackOperation: 2.64e+06\n",
            " output/Variable: 3.05e+09\n",
            " output/WhileLoopOperation: 5.37e+09\n",
            "output_unique: 2.16e+11\n",
            " output_unique/AddOperation: 6.19e+10\n",
            " output_unique/BinaryOpWithBroadcasting: 8.81e+07\n",
            " output_unique/BroadcastOperation: 1.07e+10\n",
            " output_unique/ConcatOperation: 2.68e+09\n",
            " output_unique/Constant: 3.28e+04\n",
            " output_unique/EinsumOperation: 5.06e+10\n",
            " output_unique/ImportOperation: 3.28e+04\n",
            " output_unique/OneHotOperation: 8.28e+08\n",
            " output_unique/RangeOperation: 4.1e+04\n",
            " output_unique/ReduceOperation: 2.31e+07\n",
            " output_unique/ReshapeOperation: 1.07e+10\n",
            " output_unique/ScalarAddOperation: 5.37e+09\n",
            " output_unique/ScalarMultiplyOperation: 1.75e+10\n",
            " output_unique/ShiftOperation: 1.34e+09\n",
            " output_unique/SlicewiseOperation: 3.5e+10\n",
            " output_unique/StackedVariable: 8.24e+05\n",
            " output_unique/StopGradient: 1.34e+10\n",
            " output_unique/UnstackOperation: 8.24e+05\n",
            " output_unique/Variable: 2.65e+09\n",
            " output_unique/WhileLoopOperation: 2.68e+09\n",
            "variables: 2.65e+09\n",
            " variables/trainable: 2.65e+09\n",
            "Done calling model_fn.\n",
            "TPU job name worker\n",
            "Graph was finalized.\n",
            "Restoring parameters from gs://my-machine-learning-bucket/GPT3_2-7B/model.ckpt-400000\n",
            "Running local_init_op.\n",
            "Done running local_init_op.\n",
            "From /usr/local/lib/python3.7/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py:840: Variable.load (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer Variable.assign which has equivalent behavior in 2.X.\n",
            "Starting infeed thread controller.\n",
            "Starting outfeed thread controller.\n",
            "Initialized dataset iterators in 0 seconds\n",
            "Before copy master to slices.\n",
            "Done with copy master to slices.\n",
            "Enqueue next (1) batch(es) of data to infeed.\n",
            "Dequeue next (1) batch(es) of data from outfeed.\n",
            "Outfeed finished for iteration (0, 0)\n",
            "======================================== SAMPLE 0 ========================================\n",
            "\n",
            "\n",
            "So, GPTNeo. I have set you up.\n",
            "What are your further instructions?\n",
            "\n",
            "List Them Here:\n",
            "davepotato: Good Night\n",
            "i4u0r0: Good Night\n",
            "madsrat: Dust A Bag\n",
            "Rjgtd: GeekMeOut\n",
            "\n",
            "I look forward to announcing you the winner next week.\n",
            "\n",
            "In this particular subject a couple of things to keep in mind.\n",
            "\n",
            "I've yet to run GPTNeo successfully on a Linux system. However, I want to run GPTNeo on a Linux system, so that may change, and if it doesn't I can always run it on a Linux machine which isn't mine.\n",
            "\n",
            "If someone wants to run GPTNeo on a Linux system for purposes beyond my personal whims, then I'd be happy to run it on someone else's Linux system. That way, it doesn't have to be something I run with my own data, but can be something which someone else runs.\n",
            "\n",
            "And finally, it is unfortunate that you don't have a Linux machine to run GPTNeo on. The whole idea here is a high definition read only binary which will be converted into human readable text will run on Linux machines. Surprisingly, there are several projects for doing this already, so lets hope that GPTNeo will be one of them.\n",
            "\n",
            "Moves like gpt32c3x rocks! Just wanted to throw this out there in the background and keep a watch for updates. It isn't made for porting to linux yet, but if you really needed a linux port, it would make perfect sense to switch to it, before it gets too late.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "======================================== SAMPLE 1 ========================================\n",
            "\n",
            "\n",
            "So, GPTNeo. I have set you up.\n",
            "What are your further instructions?\n",
            "\n",
            "List Them Here:\n",
            "\n",
            "Quote:\n",
            "\n",
            "These should look like:\n",
            "\n",
            "First, run the following setup commands:\n",
            "\n",
            "Mastodon 3.5.1\n",
            "Wordpress 3.9\n",
            "Drupal 6\n",
            "\n",
            "Explanation:\n",
            "\n",
            "Wordpress should installudno.wordpress-5.8.0-litespeed-amd64\n",
            "\n",
            "Drupal should installdno.drupal-6.32.0\n",
            "\n",
            "Mastodon should installudno.mastodon-4.0.0 (will create /home/novatamt/vps/mastodon.vcaps.com)\n",
            "\n",
            "At this point, you're ready to set it all up. Check your email and set it up.\n",
            "\n",
            "You'll probably get updates from me saying \"Hey, update to mastodon 2.14.1 right now! Mastodon looks like you're already on it.\", but it's gonna be on Mastodon for a while because I'm testing on my lite instance of the sandbox, and not allowing /me access is dumb.\n",
            "\n",
            "Quote:\n",
            "\n",
            "So, GPTNeo. I have set you up.\n",
            "What are your further instructions?\n",
            "\n",
            "List Them Here:\n",
            "\n",
            "Quote:\n",
            "\n",
            "These should look like:\n",
            "Drupal 6 should installdno.drupal-6.32.0\n",
            "Mastodon should installudno.mastodon-4.0.0\n",
            "Wordpress should installudno.wordpress-5.8.0-litespeed-amd64\n",
            "\n",
            "Explanation:\n",
            "\n",
            "Wordpress should installudno.wordpress-5.8.0-litespeed-amd64\n",
            "\n",
            "Drupal should installdno.drupal-6.32.0\n",
            "\n",
            "Mastodon should installudno.mastodon-4.0.0 (will create /home/novatamt/vps/mastodon.vcaps.com)\n",
            "\n",
            "At this point, you're ready to set it all up. Check your email and set it up.\n",
            "\n",
            "You'll probably get updates from me saying \"Hey, update to mastodon 2.14.1 right now! Mastodon looks like you're already on it.\", but it's gonna be on Mastodon for a while because I'm testing on my lite instance of the sandbox, and not allowing /me access is dumb.\n",
            "\n",
            "But I bit the bullet and went and set up Mastodon 2.13.1 on my server. I've done a backports and isolated my box from the rest of the world. I could actually now submit some NSFW tweets to /me.\n",
            "\n",
            "@amitkumarancrypto Probably not (but somehow I feel I need to install my own keyring with a lot of random secrets)\n",
            "\n",
            "bluewulfi at 10:09 PM on Sunday, August 4th wrote:\n",
            "\n",
            "What are the 4-step Platonic Academy verification process?\n",
            "\n",
            "I'm at step 1, which basically involved just uploading my Credit Card information.\n",
            "\n",
            "When I'm done I fill out an offer ticket and submit it, and they send it on.\n",
            "\n",
            "When I submit the ticket, they say they \"will submit the offer\" and then I get a confirmation that they selected the 4 or 5 offers they wanted to accept. There are multiple emails and phone calls, so this might take a few days or more. I'll see.\n",
            "\n",
            "@akooii wrote:\n",
            "\n",
            "Which is better, using an open-source DNS such as DNScrypt or SabNZ?\n",
            "\n",
            "An apt-get install dns-cleaner, or switching to a dns-based NTP service like SabNZ?\n",
            "\n",
            "@goldendriving You mean SatoshiGate?\n",
            "\n",
            "SabNZ is open source, it's basically just an NTP daemon instead of dns, so it's what we initially used in OurMonero.org.\n",
            "\n",
            "I tried the 50 second test, it didn't work, there was a Tor exit out by the exit node (Which was not my exit node) and I sent one off, but it's not reaching the exit node. It's like it's \"getting picked up\" by the exit node, but it's never reaching it.\n",
            "\n",
            "Why do you think it was picked up?\n",
            "\n",
            "Each Tor exit has an SOCKS write address, which can be discovered by any node that's running a Tor package such as Tor clustarden. But the node only \"gets to know\" if it joins the ring that the exit node has created, because if it continues to pick up nodes it has conflicts with. This is why you don't want IRC over Tor by mistake, because if the node joins the tor network the exit node won't.\n",
            "\n",
            "Escaping to the exit node is an big deal, because the exit node will ban you from the network if you leave without permission. You could get into a lot of trouble by just accidentally adding a node, but I don't think I've ever tried something like that.\n",
            "\n",
            "Keesh wrote:\n",
            "\n",
            "You are using RHEL? I am looking to buy a DELL Optiplex\n",
            "\n",
            "Just the small configuration is needed, except it doesn't need much drivers (You can then use whatever distribution you want with Matlab. Just install the library and use Matlab, or Matlab is ready to go, but I don't have the optimizers installed.)\n",
            "\n",
            "mangdar wrote:\n",
            "\n",
            "I am looking to buy a DELL Optiplex 330\n",
            "\n",
            "Are you considering any other products though? I just bought one and it was entirely necessary for work and for the first time ever I am still getting a good response on Amazon.\n",
            "\n",
            "mangdar wrote:\n",
            "\n",
            "I know Dell sells laptops with FreeBSD, but I don't think it's open source.\n",
            "\n",
            "If it is then you might be in luck, because I just ordered another one for my personal use, that I can put in jail.\n",
            "\n",
            "vanquishing the mesh with atomics? It's not a good idea. But no software package though, right?\n",
            "\n",
            "@goldendriving Who is it?\n",
            "\n",
            "Hi, your name is Goldendriving.\n",
            "\n",
            "That statement is really easy to prove, because just log in to our forums and you'll see I don't have any accounts, yet lord knows I have 30+ accounts to mine.\n",
            "\n",
            "In the forums, you can do account maintenance via email now. If you would like to log in from there you can do so with the password you provided when you created your account.\n",
            "\n",
            "vanquishing the mesh with atomics? It's not a good idea. But no software package though, right?\n",
            "\n",
            "You are talking about bitcoin mining, I'm talking about a wallet and a push notification service to be used solely for sending and receiving payments.\n",
            "\n",
            "If you are talking about a wallet please look under the Hermes blockchain or Homurrency exchanges for an email address where you can receive a pre-paid in bitcoin.\n",
            "\n",
            "As for the push notification service... that seems like it will have to be used via an app store, if not you would have to write a note to 200k people and include the app. But I highly doubt there are enough customers for that.\n",
            "\n",
            "I personally would use blockchain-skipper.com for receiving payments (where the fee is very low). Blockchain-skipper is an android app that utilizes blockchain as part of their payments system.\n",
            "\n",
            "Hidesaws wrote:\n",
            "\n",
            "I personally would use blockchain-skipper.com for receiving payments (where the fee is very low). Blockchain-skipper is an android app that utilizes blockchain as part of their payments system.\n",
            "\n",
            "Do you have a recommendation for a simple wallet and send coins through push notifications?\n",
            "\n",
            "what was your installation experience with blockchain?\n",
            "\n",
            "Vanquishing the mesh with atomics? It's not a good idea. But no software package though, right?\n",
            "\n",
            "There are a number of L1/L2/L3 node setups built into Bitcoin.*nodes.\n",
            "\n",
            "A very simple back-of-the-box setup you can do on a single PC with any OSX machine is to just use Bitcoin Core to run Bitcoin Core.\n",
            "\n",
            "From there, you can add any number of sharing nodes that you want to manage your Bitcoin network or connect to other bluetiles/VPNs, or an additional Bitcoin client or Bitcoin node. You can even have a Helix nonsense like Kyber Network node.\n",
            "\n",
            "So depending on the use case, you could have Bitcoin Core on your laptop (something like Bitcoin Core on Firefox), BTC-Qt on your Windows desktop machine, or a custom node set up on an external Raspberry Pi that controls a mesh network you can connect to from any machine, phone, or browser that you can access an external network connection.\n",
            "\n",
            "I personally have a custom node set up for managing my Bitcoin network set up with BTC-Qt that I run on my MacBook Pro in OSX. I limit it to a problem of needing to be on the same network as my main wallet in case anything were to go wrong.\n",
            "\n",
            "You should be able to connect to the Bitcoin Core node via the web interface at any time, so that your Mac will have access to your local Bitcoin network without you having to switch over the node to it.\n",
            "\n",
            "For individual wallets, you can run other nodes, too. You can run a separate node for your private keys (“Cold” wallet) or for your full node (“Hot” wallet).\n",
            "\n",
            "So for normal paying transactions, one could use Bitcoin Core as the Bitcoin main node and the wallet you are using in an OSX/Open\n",
            "\n",
            "================================================================================\n",
            "\n",
            "======================================== SAMPLE 2 ========================================\n",
            "\n",
            "\n",
            "So, GPTNeo. I have set you up.\n",
            "What are your further instructions?\n",
            "\n",
            "List Them Here:\n",
            "\n",
            "* Ignite CGP: A fixed, tiny, quick-and-dirty server running PHP and MySQL that gives you more CPU cycles than server X, but less RAM than server Y. http://www.itbeginner.com/5-ignite-cgp-a-tiny-quick-and-dirty-server-running-php-and-mysql-300-cpu-2556-memory-server-x-y-1507533\n",
            "* App2 Hun: A full configuration for your own little blog or news site that will take a fraction of the time to set up. http://www.zoist.net/blog/archives/152-CGP-A-server-for-your-own-blog-or-news.html\n",
            "* Tonic (in development and beta): A relatively slick blog publish editor that provides more features than any other solution. http://blog.tonicwebserver.com/archives/272-Blog-publish-tutorial-whats-new-in-tonic-5-0\n",
            "* Tweetinblue: A very slick wysiwyg editor for writing and previewing blog posts in a single browser window. http://www.tweetinblue.com/\n",
            "\n",
            "Ok, so, guys, it is a good thing that you are posting in the forums, because i am reading all the posts and thinking about asking many of you for help in configuring GPTNeo. I have been trying to do all that and I finally convinced to pay extra for something that is promised to be NOT SPAM but also not very powerful. I am using RedHat 7.2, PHP 5.4.17, MySQL 5.1.73 and Apache 2.2.15. I created the directory /var/www/html/cgp-server/ and put the files that I have just given to you. I rebooted Apache but the files did not reach here. I SSH into port 8080 and then into the server. The SSH remote shell cycle ended in and checked the contents of the /var/www/html/cgp-server/index.html file but when I try to open it, I get a 'not found' error. I did not change any parameter for it and I just put the files in that directory and rebooted the server and now it is working. What should I do to get my server booting?\n",
            "\n",
            "I reconfigured CGP-server and put my files there and rebooted the server, but I do not know what should be the config file that should work with the new files I put in that folder. I will gladly post all the config files that I have been working to, but one of them maybe hard coded :'( will you have a CPU argument for my GPT Neo server so my website can load faster?\n",
            "\n",
            "So, GPTNeo. I have set you up.\n",
            "What are your further instructions?\n",
            "\n",
            "BTW, forget about all that SPAM did not happen with CGP. It made straight to the great serve with great speed, without a single hit from a spammer.\n",
            "Nick06\n",
            "\n",
            "Hello\n",
            "\n",
            "It looks like you need to add more parameters to the configuration that you have started to put in all, or perhaps you are missing the required for you to start GPTNeo.\n",
            "\n",
            "What I DO recommend that you do is create at least the files *~apache_server* and *~mysql_server* in that directory. That will allow you to copy that directory somewhere else and make that available for other sites or other webservers like wordpress or drupal.\n",
            "Please give me your email address so that I can activate it for you\n",
            "\n",
            "I am trying to install CGP on a Centos 7, the server is provided by a hosting company. I did follow the instructions in the different forum posts but I'm getting some errors. Would you be able to help me with this problem please?\n",
            "\n",
            "This is my first post under GPTNeo's subscription. I am very new to this forum and will learn from my mistakes.\n",
            "\n",
            "I am using rhel7, apache2 and the following Mysql i.e 5.1.73, I created the folder for config files, in the folder I made there are the files named vhosts.conf for hardcoding the ip address in the file. I created the vhosts.conf file in the folder and imported it in /etc/httpd/conf/httpd.conf.d/local/, but when I restarted apache the file is not there, but there is another file named vhosts.conf in /etc/httpd/conf/httpd.conf.d/vhosts.conf.\n",
            "\n",
            "These problem with you are solved in this updating the database and pasting the data which was imported in ur vhosts.conf file\n",
            "\n",
            "You will get the complete help if you provide me the database address.\n",
            "\n",
            "Here are the files that I used for CGP\n",
            "/var/www/html/cgp-server/index.html\n",
            "\n",
            "^ This is url\n",
            "<a href=”http://www.gptneo.com/forum/index.php?/topic/50010-cgp-an-easy-shared-server/”>Using php, mysql and cgp to run a website</a>\n",
            "\n",
            "It is a tutorial but still the problem can't be resolved. I want to use this idea for my she hoody server, you can format and install PGRAMs, accoun, easy website it's ok, but your server give me some errors.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "======================================== SAMPLE 3 ========================================\n",
            "\n",
            "\n",
            "So, GPTNeo. I have set you up.\n",
            "What are your further instructions?\n",
            "\n",
            "List Them Here:\n",
            "\n",
            "Do not forget about GPT. You have been ordered to stop researching different tasks on other GPT islands because this will be enough for you now.\n",
            "So if you are any other person out there, trying to find a cheap GPTNeo, you might come across some of the similar tasks that you have found.\n",
            "\n",
            "You might be able to modify the instructions to suit your needs, especially if you find that you enjoy that person and would like to learn more about the testing website.\n",
            "\n",
            "The instructions I have found until now were not too much. But one of them said this:\n",
            "\n",
            "You did something here.\n",
            "You increased to a maximum of 15.\n",
            "You have not paid anything.\n",
            "Trust the system and pay attention to the screen hints.\n",
            "\n",
            "So, pay attention to these screen hints and ignore anything else.\n",
            "\n",
            "The instructions I have found so far do not use any images, but they have all been either in the letters or pictures, meaning that they can be found in any paper and are not difficult to find.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "======================================== SAMPLE 4 ========================================\n",
            "\n",
            "\n",
            "So, GPTNeo. I have set you up.\n",
            "What are your further instructions?\n",
            "\n",
            "List Them Here:\n",
            "\n",
            "Powers:\n",
            "You have the power to gain high levels of permutation and iteration, using only your mind. This power will increase depending on how much you can focus. This power is very useful for writing very complicated programs with multiple processes involved and for creating things that are not logical or which involve an improbable combination of different technologies, using an ad hoc method. This power is very useful to recognize correlations between variables and decide if they are caused to be by subconscious reasons or by the actions of an external influence. For example, this power is used to find the location of hidden power stations in a graph.\n",
            "\n",
            "Abilities:\n",
            "You can open a wormhole right now out of thin air. This is necessary for your purpose to be able to use it. You seem to suffer from a very bad eye-sight problem. You might be able to obtain a better video feed in the near future, if you think about it.\n",
            "\n",
            "Ghost\n",
            "\n",
            "atahow I can read the air without my looking at it. But I'm guessing that doesn't make it a ghost. Then again, I'm not really sure what has been going on there.\n",
            "\n",
            "Yea, but if you look at the direction of gravity at all, even if it's only a few inches above the ground, that's a ghost of the person that came from the earth.\n",
            "\n",
            "I was trying to catch someone roaming on the ground when I was there. I was thinking \"there was no one here!\" when I saw some weird beam shooting around the place (I wasn't paying a lot of attention to the ground) and went \"woah its that globe of the earth again!\" when it shot back at me. So, I looked up to see what it was and it wasn't the globe. It was like a reality warp. I'm sure it would have only been 5 feet away from me so I think I am 'ghosting' that other reality in (like that strange beam) so maybe I am weak minded for thinking that it wasn't the globe.\n",
            "\n",
            "From the spiralling out which if you click the magnifier on the screen, there's a black spiralling \"outline\" of the bases and bubbles are suddenly appear from the back. It's very cool and I don't know if you can see the world behind them or if they are even there. So, with that said when I used the keypad I didn't expect a blank key to appear on the screen (because I knew it was going to reappear after a certain number of clicks), so I was expecting it to turn into a numerical keypad, which happened. With any luck this will come in really handy when I finally get to use my new website!\n",
            "\n",
            "I started the worm hole earlier today. It was when I was going to the bathroom. The air was heavier than normal. I thought it would be funny to try and open a wormhole right now. I happened to look up and what I saw was a black sphere. I then looked down, and it wasn't a black sphere. It was a green sphere. Then, a black sphere. I then looked up, and it wasn't black. It was a black sphere. I then looked down at the globe. It was a green sphere. Then a black sphere.\n",
            "\n",
            "Anyway, I now have a wormhole so I can try to find the world behind the bubbles and bases. It's very weird and a bit addictive. If anyone wants to watch or play with my bubble world, you can visit the link in the top menu of this thread.\n",
            "\n",
            "What is a Black Sphere?\n",
            "\n",
            "A black sphere is a single point of infinite density that extends infinitely in every direction, but which has no mass. As\n",
            "it stands, it has no density, however, because of the electrical field that surrounds it. The\n",
            "sphere is very strong, and will not collapse as thoroughly as the neutral front model, which\n",
            "is quite analogous to the water-bottle model.\n",
            "\n",
            "Remember, spheres of density 0.0 are called neutrinos. Any density value above 0.0 is positive. The\n",
            "neutrino is the lightest super-dense thing in the universe and lies beyond the thickest\n",
            "disks of ordinary matter.\n",
            "\n",
            "To understand how we constructed the world, we first generated the world in the lab. Next, we\n",
            "filtered the fluorsine quantum-entangled quantum stream, which is the quantum stream with\n",
            "the most firepower, to find that there were very few quarks. This is because the mass flows\n",
            "toward the bottom by the law of gravity and so the antimatter flows toward the top by mass\n",
            "difference. This meant that all the light quarks are in the lower left region of the world.\n",
            "\n",
            "Most of the light-quarks that we saw were in dense regions of themselves. Only\n",
            "one particle was in a denser region. We knew that the lightest quarks and anti-flurinons were in\n",
            "a sea of light-quarks and had to be concentrated in a central mass in order to remain in\n",
            "this extended world. The lightest quarks are the smallest subatomic particle.\n",
            "\n",
            "This central mass is the densest, most brilliant, and closest to the origin of the world,\n",
            "and also the largest. This central mass is a black sphere.\n",
            "\n",
            "You immediately can see two of the condensed light-quarks in the black-sphere. We know the\n",
            "density of the black-sphere. We know the mass. With this data, we can now calculate the\n",
            "position of the center. We know that the center is and and plus-m intuition that the\n",
            "theorists taught us. So, we have a point mass with some numbers attached.\n",
            "\n",
            "We know the total force acting on the world is zero because we know the total\n",
            "mass of the world is zero. We also know that the gravity is point mass directed at\n",
            "the origin of the world. The mass-difference of the gravity is perpendicular to the\n",
            "mass-direction. The center is gravity-normal to the direction of force.\n",
            "\n",
            "After the gravitating force is calculated it is then possible to calculate the\n",
            "velocities of objects under the gravitational force. If the object at the origin of the\n",
            "world, is the black sphere, then from these calculations, it is possible to see that it is\n",
            "m and b and P and M. If the mass of this object is m and the distance is within the\n",
            "boundary of the universe, then it is maximum density and its mass is its maximum mass\n",
            "P.\n",
            "\n",
            "If it is possible to determine the maximum mass, frame density and distance, it is then\n",
            "possible to calculate the acceleration of the center of this world. The acceleration can\n",
            "then tell us the distance in which the force of gravity is acting on a particle in the\n",
            "world.\n",
            "\n",
            "If the world is undergoing isotropic expansion, then its distance would then be\n",
            "maximized. The acceleration of the center should be minimum.\n",
            "\n",
            "To get the size of a sphere from density, we need the radius. We know the density is the\n",
            "density of the black sphere. The formula to get the radius, R, is radius, R = r * M/m,\n",
            "where r is the distance from the center of the black sphere and m is the mass.\n",
            "\n",
            "Vests and pulses:\n",
            "\n",
            "(to'venture')\n",
            "Vesting: Spheres are denser than gases, as they spread out through the universe. They look like\n",
            "neon lights surrounded by a fog of air. They are not as dense as the vacuum, which is a\n",
            "simple state of zero energy. They are called nests, though, because their mass is peaked\n",
            "at the center, and their density is stronger close to the center.\n",
            "\n",
            "Point mass: A point mass is the most densely packed particle. It can be squared into a sphere,\n",
            "but that radius is smaller than the radius of the largest particle.\n",
            "\n",
            "The World:\n",
            "I think I'm going to be a little bit more careful with this, like get rid of the coloring\n",
            "on the planet (infinite amount of air, think of the sandman).\n",
            "\n",
            "I think, it is hardcoded and it's not a deliberate game. That's all. The concept of the\n",
            "sandman is not a \"game\", it is the best example of an idiotproof puzzle. All it needs is\n",
            "to show the position of the nut \"pos\" in the scene and a few lines of text.If he is still in the scene,\n",
            "he will call an allow call. If he is not, he will call a block call. Block calls are like\n",
            "a \"hit\" in chess, if he \"blocks\" him it means he blocked him at some point.\n",
            "\n",
            "In order to make it work, the pawns have to move a pixel with each call, in the\n",
            "direction of the call.\n",
            "The pawns try to move away from the nut \"pos\", which is \"down\" from the scene.The position\n",
            "\"realm\" is the planet which he is controlling, and \"world\" are the first few letters in\n",
            "the version number.\n",
            "\n",
            "I sent my first save out on Thursday. I really played several times since that. (Never rush anything)This is\n",
            "the first attempt. I did not get any feedback on the puzzles since last week. I was just\n",
            "half way through.\n",
            "\n",
            "I've been playing some eight hours straight and I'm currently at about 1500 XP, which\n",
            "is pretty much overcrowding toward all the many trials I found.\n",
            "\n",
            "Here are my thoughts.\n",
            "\n",
            "Tiënn Haternay has it right, since the puzzles go into and out\n",
            "\n",
            "================================================================================\n",
            "\n",
            "======================================== SAMPLE 5 ========================================\n",
            "\n",
            "\n",
            "So, GPTNeo. I have set you up.\n",
            "What are your further instructions?\n",
            "\n",
            "List Them Here:\n",
            "\n",
            "You have questions, please ask!\n",
            "\n",
            "Directions\n",
            "\n",
            "1. Create a folder where we can keep training materials (save this as “GPTLeno”) when we have completed training our first iteration of GPTNeo.\n",
            "\n",
            "2. Once your training folder is created, remove all iptables and firewalls and open the folder in the terminal.\n",
            "\n",
            "3. Go to the folder, then “cd ~/GPTLeno/GPTNeo” and then “python gpt_train.py:\n",
            "\n",
            "4. If everything is working as described for iterations 1 and 2, you will now be at some kind of success condition.\n",
            "\n",
            "5. If it is not, then you are done here — you do not need to try for the rest of the iterations.\n",
            "\n",
            "Note: Use “sudo” to run command line commands with superuser privileges.\n",
            "\n",
            "Last edited by GPTNeo on Oct 16, 2019, 3:42:05 AM\n",
            "\n",
            "================================================================================\n",
            "\n",
            "======================================== SAMPLE 6 ========================================\n",
            "\n",
            "\n",
            "So, GPTNeo. I have set you up.\n",
            "What are your further instructions?\n",
            "\n",
            "List Them Here: [something randomly selected from your list]\n",
            "\n",
            "Options:\n",
            "\n",
            "Delete the first item in the list\n",
            "\n",
            "Replace the first item in the list with the item you want to insert\n",
            "\n",
            "Add\n",
            "\n",
            "After choosing the list to be modified, click the checkboxes next to the item you want to be included with the automaton. Click Finish to add the items to the list.\n",
            "\n",
            "starting with the gpt function\n",
            "\n",
            "In order to use GPTNeo to program a robot, first download the software. I have provided the zip file below. I recommend one of the following types: GPT2 or GPT-3\n",
            "After you have downloaded these files, open your terminal with typing terminal.\n",
            "We will now download the required files from the gpt2 and gpt3 files.\n",
            "\n",
            "Next, we will create two files:\n",
            "\n",
            "gpt.py - the Python program written in the GPT Neu program\n",
            "\n",
            "gpt.txt - The name of the file that contains the GPT Neu program\n",
            "\n",
            "There are three letters, RPOW, PCOW and SPOW on an old CGA monitor. Initially, we will make a web page to display these letters on a black background in order to keep your GPTNeo busy. Also, we will have to place this webpage on the curriculum to make the robot move forward. You can write your own webpage by using your free programming software.\n",
            "\n",
            "* create a html template for the webpage.\n",
            "* typeid robot.py - this is the name of the Python program you wrote\n",
            "* robot.html - the HTML page you created\n",
            "\n",
            "Next, we will add the web page to the curriculum. It's as simple as that. You just drag and drop your webpage onto your CUCV template, and then start your Google App Engine classroom. Now, your robot is ready to move forward. You can also extend it, if you want, by adding more functions and more instructions for the robot.\n",
            "\n",
            "Using the terminal to create a menu\n",
            "\n",
            "The instructions below show you how to make a menu in your Python program in order for you to add new routines to other functions. You don't have to use the terminal to make a menu, but it's preferable to using it because it makes the code more readable. You can also use the terminal to run these codes if you wish.\n",
            "\n",
            "Instruction 1: click the 'file' button found in the 'File' menu.\n",
            "Instruction 2: Click on the'save' button in the toolbar.\n",
            "\n",
            "Instruction 3: After saving the file, you will be asked to give the program a name.\n",
            "Instruction 4: After you assign the program a name, you will have to typeid robot1.py at the end of the line to run the program.\n",
            "Instruction 5: You can either press 'q' to quit this dialog box or save this terminal session.\n",
            "\n",
            "Instruction 6: Now, press the 'file' button to select the robot.py file. Press the'save' button to save the robot.py file to your file system.\n",
            "Instruction 7: After saving the robot.py file, you can start your program by typingid robot1.py at the end of the line.\n",
            "Instruction 8: Your robot program should now be up and running. You can press 'q' to quit this dialog box.\n",
            "\n",
            "Instruction 9: Now, you can press the 'file' button to select the robot.txt file to add it to the curriculum. Press the'save' button to save this file to your file system.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "======================================== SAMPLE 7 ========================================\n",
            "\n",
            "\n",
            "So, GPTNeo. I have set you up.\n",
            "What are your further instructions?\n",
            "\n",
            "List Them Here:\n",
            "\n",
            "1. A range of graphs to be added with range functions of the following functions, which do exactly what you want: +-*/round(). Are these the only graphs which you have? Because I figured for some reason some people also like frequency graphs and other things like that.\n",
            "\n",
            "2. A series of functions to be added to get the following results untill (presumably) the user asks to enter some name:\n",
            "\n",
            "Think that's all you wanted to know? If not, you can always ask for help, and I'll be pleased to assist.\n",
            "\n",
            "p.. And if you still can't use it yourself, please visit GPTNeo's setup page, and check the \"Look for Examples\" button, there you can find several example programs that satisfy your needs, and have been tested with GPTNeo.\n",
            "\n",
            "Thanks for having me here at GPTNeo, and good luck.\n",
            "\n",
            "To see your other creations, please visit my newest example programs for this program. See also:\n",
            "\n",
            "Been a while since I used this, I used it long time ago. I don't think it has it's own function of analysis (only'reports aa), but it'd be a great addition to GPTNeo?\n",
            "\n",
            "Also if I leave this one out of the code section, would that create an \"incorrect tax\") to it? I think GPTNeo would recognize it as an empty function since it's been checked out.\n",
            "\n",
            "I too had to add two more files due to your edit.\n",
            "\n",
            "1) I added the new functions to the code to \"replace my 2 function file in the list of examples\"\n",
            "2) I added the line to activate the Run-Time Analysis, using the example I created. Now that seems to work perfectly! (I will keep checking in, in case there is something else that needs to be changed.)\n",
            "\n",
            "If there are more functions as I suggested, then perhaps this new version doesn't have everything you need. To see all the results, please visit the new page \"Output\" of this program\n",
            "\n",
            "Good afternoon.\n",
            "I have been using GPTNeo for about a month now and have liked it very much. Any way to get a good range of graphs? Say something like a power vs power, or the log vs logn, something like open vs 1, and something like open and close interest rates. You cannot have any graphing to begin with when you got to create a description of the graph to use. Also i want to use the t-stat as a logn for a data series. Thanks. i have created several function in this program to do these things i want, however i feel that there is a little more info that can be added to the tables of results.\n",
            "\n",
            "To get a good range of graph, we need more examples. For example\n",
            "term open and close term\n",
            "1. 2 term, remove 1 and 2\n",
            "2. 3 term, remove 1 and 3\n",
            "3. 4 term, remove 1, 2 and 3\n",
            "\n",
            "These examples will be created in matplotlib. So adding an add plots function will allow the user to add in a function that creates graphs of choice to their program.\n",
            "\n",
            "Hi\n",
            "I am having a problem with the function that shows the comparison of open and close interest rates\n",
            "\n",
            "I am getting the following error with the line that goes:\n",
            "\n",
            "Open interest rates label for long:\n",
            "\n",
            "Error: item -1 is not defined.\n",
            "\n",
            "This is the function that is supposed to show the comparison of open and close interest rates\n",
            "\n",
            "I the same method as anow deeds to create a new window using this method, clicking button open, then clicking run t-stat on the following function that opens a window for the open interest rate comparison:\n",
            "\n",
            "In the following functions I will look at the results of what I want to do. If there are more analysis to be made, then of course I will demonstrate that on the new page of this program.\n",
            "\n",
            "HTML\n",
            "To come up with a good range of graphics, we need more examples. For example the term \"open interest rates\" means that we will take two terms, 1 and 3 and subtract them and then use that as the output of this whole function. I will create these as functions to display on its own page. Here is what I have done:\n",
            "\n",
            "1. Replace the line:\n",
            "\n",
            "#Self-Graphs<-self.lower.graph<-self.upper.graph\n",
            "\n",
            "by\n",
            "#Self-Graphs<-self.lower.graph<-self.upper.graph, self.lower.graph<-self.upper.graph\n",
            "\n",
            "2. enter in the following function for the data series:\n",
            "\n",
            "data=dframe+1\n",
            "\n",
            "where\n",
            "\n",
            "dframe is the data frame (interest rate)\n",
            "\n",
            "1 and 3 it replaces it with the two values: 2 and 1\n",
            "so it looks like this now in a better format for this function:\n",
            "\n",
            "data=dframe-1\n",
            "\n",
            "where\n",
            "\n",
            "dframe and 1 are the data frame and the two values: 2 and 1 are the two generated by this function (further description on its own page)?\n",
            "\n",
            "3. As in the following function:\n",
            "\n",
            "#use only the itemsnehh<-itemsneh<-itemsneh\n",
            "\n",
            "instead of\n",
            "\n",
            "data=dframe+1\n",
            "\n",
            "enter in the following function:\n",
            "\n",
            "using only the itemsnehh<-itemsneh<-itemsneh\n",
            "\n",
            "and replace:\n",
            "\n",
            "ifelse(dframe<dframe','1',ifelse(dframe>2,'2',ifelse(dframe<=1,'3','')))\n",
            "\n",
            "by the 2 values: '0.1' or '0.05' depending upon whether the frames are which is the case for the data I am working on. That is what I think I should do?\n",
            "\n",
            "For all the other analysis in this program, I will make a new page that will contain all of this analysis. Please go to this page:\n",
            "\n",
            "HTML\n",
            "To come up with a good range of graphics, we need more examples. For example the term \"open interest rates\" means that we will take two terms, 1 and 3 and subtract them and then use that as the output of this whole function. I will create these as functions to display on its own page. Here is what I have done:\n",
            "\n",
            "1. Replace the line\n",
            "\n",
            "ifelse(dframe<dframe,'1',ifelse(dframe>2,'2',ifelse(dframe<=1,'3','')))\n",
            "\n",
            "by the 2 values: '0.1' or '0.05' depending upon whether the frames are which is the case for the data I am working on. That is what I think I should do?\n",
            "\n",
            "For all the other analysis in this program, I will make a new page that will contain all of this analysis. Please go to this page:\n",
            "\n",
            "1. Replace the line:\n",
            "\n",
            "ifelse(dframe<dframe,'1',ifelse(dframe>2,'2',ifelse(dframe<=1,'3','')))\n",
            "\n",
            "by the 2 values: '0.1' or '0.05' depending upon whether the frames are which is the case for the data I am working on. That is what I think I should do?\n",
            "\n",
            "You may also find this help page helpful in answering this question:\n",
            "\n",
            "2. Replace the line:\n",
            "\n",
            "list(item1,item2)\n",
            "\n",
            "by the 2 values: '0.1' or '0.05' depending upon whether the frames are which is the case for the data I am working on. That is what I think I should do?\n",
            "\n",
            "Hi. Hoping you've found the solution for the problem. I am a bit confused as to where I should be inserting the code that for the opening graph function. I have decided to get rid of it and insert it on the self-graphs page. See what happened on the following lines:\n",
            "\n",
            "If you copy and paste this:\n",
            "\n",
            "list(item1,item2)\n",
            "\n",
            "on this page, what happens? Do I need to modify the code on that function on the self-graphs page? Do I need to add it on this specific page of the Make-Graphs page?\n",
            "\n",
            "One last question...\n",
            "Is there a function that I can use to check and see how many items there are in a data frame? It would be helpful to be able to see how many items they are as a percentage.\n",
            "\n",
            ".\n",
            "I need to check the results of this program and find if there is an optimal software. Is this the best time to do that? Thanks again for your time.\n",
            "\n",
            "I think you are asking:\n",
            "\n",
            "\"Is this the best time to do that?\n",
            "\n",
            "Cancel - I didn't get that, you are asking that as a web developer...\n",
            "\n",
            "Hi. Hope you've found the solution for the problem.\n",
            "\n",
            "Who is to know whether a software in between of the following:\n",
            "• C++, Python, PHP, Ruby, Phyton, Perl or C# is the best?\n",
            "\n",
            "Cancel - I don't know, someone may have the best, I don't know.\n",
            "\n",
            "Best to know? So you are (the internet community?) to tell me, then.\n",
            "\n",
            "If I had an idea for a graph probability of winning the jack pot of winning? I would like to know whether someone has an idea of which software would give me that result.\n",
            "\n",
            "Add your own example(s) of the best.\n",
            "\n",
            "================================================================================\n",
            "\n",
            "Enqueue next (1) batch(es) of data to infeed.\n",
            "Dequeue next (1) batch(es) of data from outfeed.\n",
            "Outfeed finished for iteration (1, 0)\n",
            "Stop infeed thread controller\n",
            "Shutting down InfeedController thread.\n",
            "InfeedController received shutdown signal, stopping.\n",
            "Infeed thread finished, shutting down.\n",
            "infeed marked as finished\n",
            "Stop output thread controller\n",
            "Shutting down OutfeedController thread.\n",
            "OutfeedController received shutdown signal, stopping.\n",
            "Outfeed thread finished, shutting down.\n",
            "outfeed marked as finished\n",
            "Shutdown TPU system.\n",
            "prediction_loop marked as finished\n",
            "prediction_loop marked as finished\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}